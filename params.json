{"name":"Mesos on docker","tagline":"Run Mesos on Docker.","body":"SLT 2014 DEMO\r\n=============\r\n\r\nHere you can find all scripts needed to run CloudASR on your computer.\r\nIn this demo, CloudASR will run 4 different workers:\r\n\r\n  - **English (VYSTADIAL TownInfo AM+LM)** - town information domain specific model.\r\n    The model is based on free data [Vystadial 2013 – English data](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-4671-4).\r\n    You can try sentences like:\r\n    - I am looking for a cheap Chinese restaurant.\r\n    - I am looking for a fast food.\r\n    - I am looking for a bar.\r\n\r\n  - **English (TED AM+Wikipedia LM)** - open domain model.\r\n\r\n    You can try sentences like these:\r\n    - Who was the first president of the United States?\r\n    - I live in the Czech Republic.\r\n    - Who was that?\r\n\r\n  - **Czech (VYSTADIAL AM + PTIcs LM)** - public transport domain specific model.\r\n    The model is based on free data [Vystadial 2013 – Czech data](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-4670-6).\r\n    You can try sentences like these:\r\n    - Chtěl bych jet z Anděla na Malostranskou.\r\n    - Jak se dostanu z Prahy do Brna?\r\n    - V kolik hodin mi to jede?\r\n\r\n  - **Czech (VYSTADIAL AM + Wikipedia LM)** - open domain model. You can try sentences like these:\r\n    - Praha je hlavní město České Republiky.\r\n    - Bydlím v Praze.\r\n    - Bedřich Smetana je slavný český skladatel.\r\n\r\n### Installation\r\nIn order to be able to run CloudASR Docker has to be installed on the host machine.\r\nYou can follow the instructions for your distribution at [http://docs.docker.com/installation/](http://docs.docker.com/installation/).\r\nAdditionally it is necessary to download docker images. You can do that by typing `make pull` - be aware that the images has several GBs.\r\n\r\n### Running the demo locally\r\nJust type `make run_locally` and everything will be running in a while.\r\nYou can open [http://localhost:8001](http://localhost:8001) to see which workers are running.\r\nAdditionally, you can open [http://localhost:8000](http://localhost:8000) and try out our interactive web demo.\r\n\r\n### Running the demo on Mesos cluster\r\nIn order to be able to run the demo on Mesos cluster, you have to update `marathon_url` and `master_ip` in the `mesos.json` configuration:\r\n```json\r\n{\r\n    \"domain\": \"cloudasr.com\",\r\n    \"marathon_url\": \"localhost:8080\",\r\n    \"master_ip\": \"127.0.0.1 - IP of the mesos-slave where the CloudASR master should run\",\r\n    \"workers\": [\r\n        {\"image\": \"ufaldsg/cloud-asr-worker-en\", \"instances\": 1},\r\n        {\"image\": \"ufaldsg/cloud-asr-worker-en-wiki\", \"instances\": 1},\r\n        {\"image\": \"ufalgsg/cloud-asr-worker-cs\", \"instances\": 1},\r\n        {\"image\": \"ufaldsg/cloud-asr-worker-cs-alex\", \"instances\": 1}\r\n    ]\r\n}\r\n```\r\n> Note that the suffix of the worker image name corresponds to `lang` parameter used in Batch and Online APIs. For `example ufaldsg/cloud-asr-worker-en` will handle requests with parameter `lang=en`.\r\n\r\nAfter that you can type `make run_mesos` and you should see running instances in the Marathon console in a while. After that you should start a load-balancer on a server associated with the domain specified in the `mesos.json`. You can do that by typing:\r\n```\r\ndocker run -p 80:80 -e MARATHON_URL=localhost:8080 -d choko/haproxy\r\n```\r\nAfter that you should be able to see the demo page on [http://demo.cloudasr.com](http://demo.cloudasr.com) and the monitor page on [http://monitor.cloudasr.com](http://monitor.cloudasr.com). \r\n\r\n## How to use CloudASR\r\nCloudASR provides two modes of speech recognition: online recognition and batch recognition.\r\nIn the following text we will describe how you can use them.\r\n\r\n### Batch API\r\nBatch API is compatible with Google Speech API, but it supports only wav files and json output at this moment.\r\nUsers can use parameter `lang` to specify which language they want to use for speech recognition. These language models are available now:\r\n  - **en-towninfo** - English (VYSTADIAL TownInfo AM+LM)\r\n  - **en-wiki** - English (TED AM+Wikipedia LM)\r\n  - **cs** - Czech (VYSTADIAL AM + Wikipedia LM)\r\n  - **cs-alex** - Czech (VYSTADIAL AM + PTIcs LM) \r\n\r\nIf you want to transcribe english speech in a `recording.wav` file you can send following curl request:\r\n```\r\ncurl -X POST --data-binary @recording.wav --header 'Content-Type: audio/x-wav; rate=16000;' 'http://localhost:8000/recognize?lang=en-towninfo'\r\n```\r\n\r\nand you should get a response similiar to this:\r\n```json\r\n{\r\n  \"result\": [\r\n    {\r\n      \"alternative\": [\r\n        {\r\n          \"confidence\": 0.5549500584602356,\r\n          \"transcript\": \"I'M LOOKING FOR A BAR\"\r\n        },\r\n        {\r\n          \"confidence\": 0.14846260845661163,\r\n          \"transcript\": \"I AM LOOKING FOR A BAR\"\r\n        },\r\n        {\r\n          \"confidence\": 0.08276544511318207,\r\n          \"transcript\": \"I'M LOOKING FOR A RESTAURANT\"\r\n        },\r\n        {\r\n          \"confidence\": 0.06668572872877121,\r\n          \"transcript\": \"I AM LOOKING FOR A RESTAURANT\"\r\n        }\r\n      ],\r\n      \"final\": true\r\n    }\r\n  ],\r\n  \"result_index\": 0\r\n}\r\n```\r\n\r\n### Online API\r\nOnline API uses Sockets.io for transfering PCM chunks to the CloudASR server. Messages have following format:\r\n\r\n#### From Client to Server\r\n  - First we have to start recognition by sending information about used language.\r\n    ```javascript\r\n    socketio.emit('begin', {'lang': 'en-GB'})\r\n    ```\r\n  - After that we can send PCM chunks to the server. Every chunk is a 16 bit PCM array.\r\n    ```javascript\r\n    socketio.emit('chunk',  {'chunk': [128, 123, 15,..., 25], 'frame_rate': 16000})\r\n    ```\r\n\r\n  - Finally we end the recognition by sending following message\r\n    ```javascript\r\n    socketio.emit('end', {})\r\n    ```\r\n\r\n#### From Server to Client\r\nServer responds to every chunk with a message with interim results:\r\n```json\r\n{\r\n    \"status\": 0,\r\n    \"final\": false,\r\n    \"result\": {\r\n        \"hypotheses\": [\r\n            {\"transcript\": \"I AM LOOKING\"}\r\n        ]\r\n    }\r\n}\r\n```\r\n\r\nAt the end of the recognition server sends final hypothesis in the following format:\r\n```json\r\n{\r\n    \"result\": [\r\n        {\r\n            \"alternative\": [\r\n                {\"confidence\": 0.5364137887954712, \"transcript\": \"I AM LOOKING FOR A MY\"},\r\n                {\"confidence\": 0.46358612179756165, \"transcript\": \"I'M LOOKING FOR A MY\"}\r\n            ],\r\n            \"final\": true\r\n        }\r\n    ],\r\n    \"result_index\": 0\r\n}\r\n```\r\n\r\n> Note that the Online API will switch from SocketsIO to binary Websockets to decrease the traffic in the near future.\r\n\r\n#### Using CloudASR's SpeechRecognition.js library\r\nIf you want to use speech recegnition on your website, you can use our javascript library. Please add these scripts to your html:\r\n```html\r\n<script src=\"http://www.cloudasr.com/js/socket.io.js\"></script>\r\n<script src=\"http://www.cloudasr.com/js/Recorder.js\"></script>\r\n<script src=\"http://www.cloudasr.com/js/SpeechRecognition.js'\"></script>\r\n```\r\n\r\nThen you can use SpeechRecognition in following manner:\r\n```javascript\r\nvar speechRecognition = new SpeechRecognition();\r\nspeechRecognition.onStart = function() {\r\n    console.log(\"Recognition started\");\r\n}\r\n\r\nspeechRecognition.onEnd = function() {\r\n    console.log(\"Recognition ended\");\r\n}\r\n\r\nspeechRecognition.onError = function(error) {\r\n    console.log(\"Error occured: \" + error);\r\n}\r\n\r\nspeechRecognition.onResult = function(result) {\r\n    console.log(result);\r\n}\r\n\r\nvar lang = \"en-wiki\";\r\n$(\"#button_start\").click(function() {\r\n    speechRecognition.start(lang);\r\n});\r\n\r\n$(\"#button_stop\").click(function() {\r\n    speechRecognition.stop()\r\n});\r\n```\r\n\r\nYou can also take a look at source code of our demo page ([index.html](https://github.com/UFAL-DSG/cloud-asr/blob/master/cloudasr/frontend/templates/index.html), [main.js](https://github.com/UFAL-DSG/cloud-asr/blob/master/cloudasr/frontend/static/js/main.js)).\r\n\r\n## Privacy & Terms\r\nAll data, including audio recording, is stored for the purpose of ASR quality improvement.\r\nNote that the data can be shared with third parties for both research and commercial purposes.\r\nAll collected data will be made available to the ASR community; therefore, do not say anything you do not want \r\nanyone to know about.\r\n\r\nThe service is available for free. As a result, no guarantees are given regarding the quality of \r\nASR results. As of now, it is a beta product; thus, things may break and the service may not be \r\navailable for large periods of time.\r\n\r\n## Contact us\r\nThe CloudASR platform is developed by the Dialogue Systems Group at [UFAL](http://ufal.mff.cuni.cz) and the work is funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221, by the core research funding of Charles University in Prague. The language resources presented in this work are stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013).\r\n\r\nIf you have any questions regarding CloudASR you can reach us at our mailinglist: [cloudasr@googlegroups.com](mailto:cloudasr@googlegroups.com).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}